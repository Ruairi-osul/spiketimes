import pandas as pd
import numpy as np
import multiprocessing
from itertools import combinations, product
from ..statistics import spike_count_correlation, spike_count_correlation_test
from ..statistics.utils import p_adjust


def spike_count_correlation_df(
    df: pd.core.frame.DataFrame,
    fs: int,
    neuron_col: str = "neuron_id",
    spiketimes_col: str = "spiketimes",
    min_firing_rate: float = None,
    t_start: float = None,
    t_stop: float = None,
):
    """
    Given a df containing one row per spike time and a neuron_id label,
    calculates the pearson correlation between spike counts.
    Spike counts are generated by binning spikes at a specified sampling rate.

    params:
        df: dataframe containing the data
        fs: sampling rate at for bins
        spiketimes_col: label of the column containing spiketime information
        neuron_col: label of the column containing neuron ids
        min_firing_rate: if specified, only correlates bins with a geometric mean firing rate
                         between neuron pairs above the specified value. 
        t_start: start point for first time bin.
        t_stop: end point for the last time bin. 

    returns:
        A dataframe containing one row per neuron pair and their
        pearson correlation coefficients. 
        Columns: {'neuron_1', 'neuron_2', 'pearson_r'}
    """

    if t_start is None:
        t_start = np.min(df[spiketimes_col])
    if t_stop is None:
        t_stop = np.max(df[spiketimes_col])
    neurons = df[neuron_col].unique()

    neuron_combs = list(combinations(neurons, r=2))
    args = [
        [
            df[df[neuron_col] == neuron_1][spiketimes_col].values,
            df[df[neuron_col] == neuron_2][spiketimes_col].values,
            fs,
            min_firing_rate,
            t_start,
            t_stop,
        ]
        for neuron_1, neuron_2 in neuron_combs
    ]

    with multiprocessing.Pool() as p:
        corrs = p.starmap(spike_count_correlation, args)

    neurons = [[n1 for n1, n2 in neuron_combs], [n2 for n1, n2 in neuron_combs]]
    return pd.DataFrame(
        {"neuron_1": neurons[0], "neuron_2": neurons[1], "pearson_r": corrs}
    )


def spike_count_correlation_between_groups(
    df: pd.core.frame.DataFrame,
    fs: int,
    neuron_col: str = "neuron_id",
    group_col: str = "group",
    spiketimes_col: str = "spiketimes",
    min_firing_rate: float = None,
    t_start: float = None,
    t_stop: float = None,
):
    """
    Given a df containing one row per spike time and neuron_id and group_id labels
    calculates the pearson correlation between spike counts of all pairs neurons in
    all pairs of groups.
    Spike counts are generated by binning spikes at a specified sampling rate.

    params:
        df: dataframe containing the data
        fs: sampling rate at for bins
        spiketimes_col: label of the column containing spiketime information
        neuron_col: label of the column containing neuron ids
        group_col: label of the column containing group ids
        min_firing_rate: if specified, only correlates bins with a geometric mean firing rate
                         between neuron pairs above the specified value. 
        t_start: start point for first time bin.
        t_stop: end point for the last time bin.

    returns:
        A dataframe containing one row per neuron pair and their
        pearson correlation coefficients and group membership.
        Columns: {'group_1', 'group_2', 'neuron_1', 'neuron_2', 'pearson_r'}
    """
    if t_start is None:
        t_start = np.min(df[spiketimes_col])
    if t_stop is None:
        t_stop = np.max(df[spiketimes_col])
    groups = df[group_col].unique()
    frames: list = []
    for group_1, group_2 in combinations(groups, r=2):
        neurons_group_1 = df.loc[df[group_col] == group_1][neuron_col].unique()
        neuron_group_2 = df.loc[df[group_col] == group_2][neuron_col].unique()

        neuron_combs = list(product(neurons_group_1, neuron_group_2))
        args = [
            [
                df[df[neuron_col] == neuron_1][spiketimes_col].values,
                df[df[neuron_col] == neuron_2][spiketimes_col].values,
                fs,
                min_firing_rate,
                t_start,
                t_stop,
            ]
            for neuron_1, neuron_2 in neuron_combs
        ]
        with multiprocessing.Pool() as p:
            corrs = p.starmap(spike_count_correlation, args)

        neurons = [[n1 for n1, n2 in neuron_combs], [n2 for n1, n2 in neuron_combs]]
        frames.append(
            pd.DataFrame(
                {
                    "group_1": group_1,
                    "group_2": group_2,
                    "neuron_1": neurons[0],
                    "neuron_2": neurons[1],
                    "pearson_r": corrs,
                }
            )
        )
    return pd.concat(frames, axis=1).transpose()


def spike_count_correlation_df_test(
    df: pd.core.frame.DataFrame,
    fs: int,
    n_boot: int = 500,
    neuron_col: str = "neuron_id",
    spiketimes_col: str = "spiketimes",
    min_firing_rate: float = None,
    t_start: float = None,
    t_stop: float = None,
    tail: str = "two_tailed",
    verbose: bool = False,
    adjust_p: bool = True,
    p_adjust_method: str = "Benjamini-Hochberg",
):
    """
    Given a df containing one row per spike time and neuron_id and group_id labels
    calculates the pearson correlation between spike counts of all pairs neurons in
    all pairs of groups. Also tests significance by generating surrogate spiketrains.
    Spike counts are generated by binning spikes at a specified sampling rate.

    params:
        df: dataframe containing the data
        fs: sampling rate at for bins
        n_boot: number of bootstrap replicates to compare against. more replicates are 
                more accurate but require more computing power to calculate. 
        spiketimes_col: label of the column containing spiketime information
        neuron_col: label of the column containing neuron ids
        min_firing_rate: if specified, only correlates bins with a geometric mean firing rate
                         between neuron pairs above the specified value. 
        t_start: start point for first time bin.
        t_stop: end point for the last time bin. 
        tail: whether to perform a two_tailed, left_sided or right_sided test
        verbose: if specified will print the id of neurons when correlating

    returns:
        A dataframe containing one row per neuron pair, their
        pearson correlation coefficients and p vaue.
        Columns: {'neuron_1', 'neuron_2', 'r', 'p'}
    """
    if t_start is None:
        t_start = np.min(df[spiketimes_col])
    if t_stop is None:
        t_stop = np.max(df[spiketimes_col])
    neurons = df[neuron_col].unique()

    neuron_combs = list(combinations(neurons, r=2))
    args = [
        [
            df[df[neuron_col] == neuron_1][spiketimes_col].values,
            df[df[neuron_col] == neuron_2][spiketimes_col].values,
            fs,
            n_boot,
            min_firing_rate,
            t_start,
            t_stop,
            tail,
        ]
        for neuron_1, neuron_2 in neuron_combs
    ]

    with multiprocessing.Pool() as p:
        corrs = p.starmap(spike_count_correlation_test, args)

    corrs = [[r for r, p in corrs], [p for r, p in corrs]]
    neurons = [[n1 for n1, n2 in neuron_combs], [n2 for n1, n2 in neuron_combs]]
    df = pd.DataFrame(
        {
            "neuron_1": neurons[0],
            "neuron_2": neurons[1],
            "pearson_r": corrs[0],
            "p": corrs[1],
        }
    )
    if adjust_p:
        df["p"] = p_adjust(df["p"].values, method=p_adjust_method)

    return df


def spike_count_correlation_between_groups_test(
    df: pd.core.frame.DataFrame,
    fs: int,
    n_boot: int = 500,
    neuron_col: str = "neuron_id",
    group_col: str = "",
    spiketimes_col: str = "spiketimes",
    min_firing_rate: float = None,
    t_start: float = None,
    t_stop: float = None,
    tail: str = "two_tailed",
    verbose: bool = False,
):
    """
    Given a df containing one row per spike time and neuron_id and group_id labels
    calculates the pearson correlation between spike counts of all pairs neurons in
    all pairs of groups. Also calculates significance value via bootstrap approach.
    Surrogate spiketrains are generated from each pair by shuffling the inter spike intervals.
    Spike counts are generated by binning spikes at a specified sampling rate.

    params:
        df: dataframe containing the data
        fs: sampling rate at for bins
        n_boot: number of bootstrap replicates to compare against. more replicates are 
                more accurate but require more computing power to calculate. 
        spiketimes_col: label of the column containing spiketime information
        neuron_col: label of the column containing neuron ids
        group_col: label of the column containing group ids
        min_firing_rate: if specified, only correlates bins with a geometric mean firing rate
                         between neuron pairs above the specified value. 
        t_start: start point for first time bin.
        t_stop: end point for the last time bin.
        tail: whether to perform a two_tailed, left_sided or right_sided test
        verbose: if specified will print the id of neurons when correlating

    returns:
        A dataframe containing one row per neuron pair and their
        pearson correlation coefficients and group membership.
        Columns: {'group_1', 'group_2', 'neuron_1', 'neuron_2', 'pearson_r'}
    """
    if t_start is None:
        t_start = np.min(df[spiketimes_col])
    if t_stop is None:
        t_stop = np.max(df[spiketimes_col])
    frames: list = []
    groups = df[group_col].unique()
    for group_1, group_2 in combinations(groups, r=2):
        neurons_group_1 = df.loc[df[group_col] == group_1][neuron_col].unique()
        neuron_group_2 = df.loc[df[group_col] == group_2][neuron_col].unique()

        neuron_combs = list(product(neurons_group_1, neuron_group_2))
        args = [
            [
                df[df[neuron_col] == neuron_1][spiketimes_col].values,
                df[df[neuron_col] == neuron_2][spiketimes_col].values,
                fs,
                n_boot,
                min_firing_rate,
                t_start,
                t_stop,
                tail,
            ]
            for neuron_1, neuron_2 in neuron_combs
        ]
        with multiprocessing.Pool() as p:
            corrs = p.starmap(spike_count_correlation_test, args)

        neurons = [[n1 for n1, n2 in neuron_combs], [n2 for n1, n2 in neuron_combs]]
        frames.append(
            pd.DataFrame(
                {
                    "group_1": group_1,
                    "group_2": group_2,
                    "neuron_1": neurons[0],
                    "neuron_2": neurons[1],
                    "pearson_r": corrs,
                }
            )
        )
    return pd.concat(frames, axis=1).transpose()

